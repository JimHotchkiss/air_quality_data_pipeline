{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as a data engineer, ensuring data integrity by preventing duplicates is crucial. Here are several best practices to ensure only unique rows get inserted into your database:\n",
    "\n",
    "### **1. Use Primary Keys and Unique Constraints**  \n",
    "- Define a **Primary Key (PK)** on a column or a combination of columns that uniquely identify each row.  \n",
    "- Use a **UNIQUE constraint** on columns that should not contain duplicate values.  \n",
    "  ```sql\n",
    "  CREATE TABLE air_quality (\n",
    "      id SERIAL PRIMARY KEY,\n",
    "      station_id TEXT NOT NULL,\n",
    "      timestamp TIMESTAMP NOT NULL,\n",
    "      pm2_5 FLOAT,\n",
    "      pm10 FLOAT,\n",
    "      UNIQUE (station_id, timestamp)  -- Prevents duplicate station readings at the same timestamp\n",
    "  );\n",
    "  ```\n",
    "\n",
    "### **2. Use UPSERT (MERGE, ON CONFLICT, or INSERT IGNORE)**\n",
    "- Some databases support **UPSERT** or `MERGE`, which allows inserting new data or updating existing rows when a conflict occurs:\n",
    "  - **PostgreSQL:**  \n",
    "    ```sql\n",
    "    INSERT INTO air_quality (station_id, timestamp, pm2_5, pm10)\n",
    "    VALUES ('ST123', '2025-01-31 12:00:00', 12.5, 25.0)\n",
    "    ON CONFLICT (station_id, timestamp) DO NOTHING;\n",
    "    ```\n",
    "  - **MySQL:**  \n",
    "    ```sql\n",
    "    INSERT INTO air_quality (station_id, timestamp, pm2_5, pm10)\n",
    "    VALUES ('ST123', '2025-01-31 12:00:00', 12.5, 25.0)\n",
    "    ON DUPLICATE KEY UPDATE pm2_5 = VALUES(pm2_5), pm10 = VALUES(pm10);\n",
    "    ```\n",
    "\n",
    "### **3. Implement Deduplication in ETL Pipelines**  \n",
    "Before inserting data, clean and deduplicate using:  \n",
    "- **SQL Query:**  \n",
    "  ```sql\n",
    "  SELECT DISTINCT * FROM staging_air_quality;\n",
    "  ```\n",
    "- **Pandas (Python):**  \n",
    "  ```python\n",
    "  df = df.drop_duplicates(subset=['station_id', 'timestamp'])\n",
    "  ```\n",
    "\n",
    "### **4. Use Hashing or Checksums**  \n",
    "- Create a **hash column** (e.g., MD5, SHA256) for data uniqueness verification:\n",
    "  ```sql\n",
    "  ALTER TABLE air_quality ADD COLUMN data_hash TEXT UNIQUE;\n",
    "  ```\n",
    "  ```python\n",
    "  import hashlib\n",
    "  df['data_hash'] = df.apply(lambda row: hashlib.md5(f\"{row.station_id}_{row.timestamp}\".encode()).hexdigest(), axis=1)\n",
    "  ```\n",
    "\n",
    "### **5. Use Change Data Capture (CDC)**  \n",
    "- If dealing with streaming data, use CDC techniques to track changes and insert only new data.\n",
    "\n",
    "Would you like help implementing any of these in your air quality ETL pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is `ROW_NUMBER()`?\n",
    "`ROW_NUMBER()` is a **window function** in SQL that assigns a unique sequential number to rows within a result set. This number is assigned based on a specified ordering.\n",
    "\n",
    "### How does it work?\n",
    "1. **Partitioning**:  \n",
    "   You can divide your data into \"partitions\" (groups of rows) based on certain columns using the `PARTITION BY` clause. Within each partition, the row numbering starts at 1 and increases by 1 for each subsequent row.\n",
    "   \n",
    "   - Example: If you partition by `location_id`, `ROW_NUMBER()` will start numbering the rows separately for each distinct `location_id`.\n",
    "\n",
    "2. **Ordering**:  \n",
    "   You also need to specify an order in which the rows should be numbered within each partition using the `ORDER BY` clause. The numbering follows this order.\n",
    "\n",
    "   - Example: If you order by `ingestion_datetime DESC`, the rows will be numbered starting from the most recent.\n",
    "\n",
    "3. **Result**:  \n",
    "   After running the query, you'll get a new column with the row numbers (`ROW_NUMBER()`) based on your partitioning and ordering.\n",
    "\n",
    "### Example\n",
    "\n",
    "Letâ€™s walk through an example to see how `ROW_NUMBER()` works.\n",
    "\n",
    "#### Sample Data:\n",
    "| location_id | sensors_id | datetime           | value |\n",
    "|-------------|------------|--------------------|-------|\n",
    "| 1           | 101        | 2025-01-31 10:00   | 10    |\n",
    "| 1           | 101        | 2025-01-31 11:00   | 15    |\n",
    "| 1           | 101        | 2025-01-31 12:00   | 20    |\n",
    "| 2           | 102        | 2025-01-31 10:00   | 30    |\n",
    "| 2           | 102        | 2025-01-31 11:00   | 35    |\n",
    "| 2           | 102        | 2025-01-31 12:00   | 40    |\n",
    "\n",
    "#### SQL Query:\n",
    "```sql\n",
    "SELECT \n",
    "    location_id,\n",
    "    sensors_id,\n",
    "    datetime,\n",
    "    value,\n",
    "    ROW_NUMBER() OVER (PARTITION BY location_id, sensors_id ORDER BY datetime DESC) AS rn\n",
    "FROM air_quality\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "| location_id | sensors_id | datetime           | value | rn  |\n",
    "|-------------|------------|--------------------|-------|-----|\n",
    "| 1           | 101        | 2025-01-31 12:00   | 20    | 1   |\n",
    "| 1           | 101        | 2025-01-31 11:00   | 15    | 2   |\n",
    "| 1           | 101        | 2025-01-31 10:00   | 10    | 3   |\n",
    "| 2           | 102        | 2025-01-31 12:00   | 40    | 1   |\n",
    "| 2           | 102        | 2025-01-31 11:00   | 35    | 2   |\n",
    "| 2           | 102        | 2025-01-31 10:00   | 30    | 3   |\n",
    "\n",
    "### Key Points:\n",
    "1. **`PARTITION BY location_id, sensors_id`**:  \n",
    "   This divides the data into groups based on `location_id` and `sensors_id`. Each group is numbered separately. So, the rows with `location_id = 1` and `sensors_id = 101` get numbered independently from the rows with `location_id = 2` and `sensors_id = 102`.\n",
    "\n",
    "2. **`ORDER BY datetime DESC`**:  \n",
    "   Within each group, the rows are ordered by `datetime` in descending order. This means that the most recent record gets `ROW_NUMBER()` = 1.\n",
    "\n",
    "3. **`ROW_NUMBER()`**:  \n",
    "   This assigns a unique number to each row within the partition, starting from 1 for the first row.\n",
    "\n",
    "### Use Case in Your Query\n",
    "In your original query, `ROW_NUMBER()` is used to:\n",
    "- **Partition** by `location_id`, `sensors_id`, `\"datetime\"`, and `\"parameter\"`.\n",
    "- **Order** by `ingestion_datetime DESC` to get the most recent data for each partition.\n",
    "- Only keep the **most recent row** for each combination by filtering `WHERE rn = 1`.\n",
    "\n",
    "### Why is this Useful?\n",
    "- **Removing duplicates**: If you have multiple records for the same combination of `location_id`, `sensors_id`, and `\"datetime\"`, you can use `ROW_NUMBER()` to pick the most recent one and discard older duplicates.\n",
    "- **Data Cleaning**: This is useful when you want to eliminate redundant data and keep only the latest or most relevant entries for analysis or visualization.\n",
    "\n",
    "Would you like to see a practical example with your data?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
